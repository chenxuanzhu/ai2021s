{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业3：深度学习框架实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次作业将练习深度学习框架的使用，大部分内容用 PyTorch 实现。第1题利用卷积层和全连接层实现手写数字的识别，第2题利用 RNN 来实现英文名的自动生成，第3题是算法题，利用卷积运算实现任意大整数的乘法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 目标：通过对 MNIST 数据进行训练，构建一个简单的图像分类模型，对图片中的数字进行识别。你将利用该模型对自己真实手写出的数字进行预测，观察模型效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 主要步骤：获取数据，创建模型结构，定义损失函数，编写训练循环，实施预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取数据。我们使用知名的 MNIST 数据集，它可以从 PyTorch 中利用工具函数下载得到。原始的 MNIST 数据训练集大小为60000，我们随机抽取其中的10000个观测进行简单的训练。以下函数会在当前目录建立一个名为 data 的文件夹，其中会包含下载得到的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：请在任何程序的最开始加上随机数种子的设置。请保持这一习惯。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "mnist = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "loader = DataLoader(mnist, batch_size=10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一次性取出随机抽取到的10000个观测，其中 x 是图片数据，y 是图片对应的数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个习惯性动作是查看数据的大小和维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以利用下面的函数展示图片的内容。如选择第一张图片，先将其转换成 Numpy 数组，再绘制图形："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = x[0].squeeze().cpu().numpy()\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来请你选择5个你喜欢的数字（10000以下），然后取出对应位置的图片，并画出它们的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot five digits here\n",
    "img1 = x[111].squeeze().cpu().numpy()\n",
    "img2 = x[222].squeeze().cpu().numpy()\n",
    "img3 = x[333].squeeze().cpu().numpy()\n",
    "img4 = x[444].squeeze().cpu().numpy()\n",
    "img5 = x[555].squeeze().cpu().numpy()\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1,5,1)\n",
    "plt.imshow(img1, cmap=\"gray\")\n",
    "plt.subplot(1,5,2)\n",
    "plt.imshow(img2, cmap=\"gray\")\n",
    "plt.subplot(1,5,3)\n",
    "plt.imshow(img3, cmap=\"gray\")\n",
    "plt.subplot(1,5,4)\n",
    "plt.imshow(img4, cmap=\"gray\")\n",
    "plt.subplot(1,5,5)\n",
    "plt.imshow(img5, cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 搭建模型。我们搭建一个类似于 LeNet-5 的网络，结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pic1.zhimg.com/80/v2-82eabb4c17e90d467197d013f7629f3c_720w.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要创建2个卷积层、2个汇聚层（池化层）和2个全连接层，**暂时忽略所有的激活函数**。所有隐藏层的函数细节都可以在[官方文档](https://pytorch.org/docs/stable/nn.html)中按分类找到。每一个隐藏层本质上都是将一个数组变换成另一个数组的函数，因此为了确认编写的模型是正确的，可以先用一个小数据进行测试，观察输入和输出的维度。例如，我们先取出前10个观测，此时输入的维度是 `[10, 1, 28, 28]`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "smallx = x[0:10]\n",
    "smally = y[0:10]\n",
    "print(smallx.shape)\n",
    "print(smally.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建第1个卷积层，并测试输出的维度。注意到我们可以直接将隐藏层当成一个函数来调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "conv1 = torch.nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "res = conv1(smallx)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，输出的维度为 `[20, 24, 24]`（不包括第1位的数据批次维度），与之前图中的结果吻合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，请按照图中提示编写层对象 `pool1`、`conv2`、`pool2`、`fc1` 和 `fc2`，并顺次测试输入与输出的维度，使其与上图匹配。注意，在将一个大小为 `[10, 50, 4, 4]` 的数组（假设叫 `somearray`）传递给 `fc1` 之前，需要先将其变形为只有两个维度的数组，做法是 `somearray.view(-1, 50*4*4)`，其中 -1 表示该位置的大小不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "pool1 = ...\n",
    "res = pool1(res)\n",
    "print(res.shape)\n",
    "\n",
    "conv2 = ...\n",
    "res = conv2(res)\n",
    "print(res.shape)\n",
    "\n",
    "pool2 = ...\n",
    "res = pool2(res)\n",
    "print(res.shape)\n",
    "\n",
    "fc1 = ...\n",
    "res = fc1(res.view(-1, 50 * 4 * 4))\n",
    "print(res.shape)\n",
    "\n",
    "fc2 = ...\n",
    "res = fc2(res)\n",
    "print(res.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "res = pool1(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "conv2 = torch.nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "res = conv2(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "res = pool2(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 800])\n"
     ]
    }
   ],
   "source": [
    "res = res.view(-1, 50*4*4)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 500])\n"
     ]
    }
   ],
   "source": [
    "fc1 = torch.nn.Linear(in_features=800,out_features=500,bias = True )\n",
    "res = fc1(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 500])\n"
     ]
    }
   ],
   "source": [
    "res = torch.nn.functional.relu(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "fc2 = torch.nn.Linear(in_features=500,out_features=10,bias = True )\n",
    "res = fc2(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "res = torch.nn.functional.softmax(res, dim = 0)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建模型类。在确保隐藏层维度都正确后，将所有的隐藏层封装到一个模型类中，其中模型结构在 `__init__()` 中定义，具体的计算过程在 `forward()` 中实现。此时需要加入激活函数。在本模型中，**请在 `conv1`、`conv2` 和 `fc1` 后加入 ReLU 激活函数，并在 `fc2` 后加入 Softmax 激活函数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = ...\n",
    "        self.pool1 = ...\n",
    "        self.conv2 = ...\n",
    "        self.pool2 = ...\n",
    "        self.fc1 = ...\n",
    "        self.fc2 = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        ...\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(in_features=800,out_features=500,bias = True )\n",
    "        self.fc2 = torch.nn.Linear(in_features=500,out_features=10,bias = True )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(torch.nn.functional.relu(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(torch.nn.functional.relu(x))\n",
    "        x = x.view(-1, 50*4*4)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(torch.nn.functional.relu(x))\n",
    "        x = torch.nn.functional.softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次测试输入输出的维度是否正确。如果模型编写正确，输出的维度应该是 `[10, 10]`，且输出结果为0到1之间的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "tensor([[0.0992, 0.0989, 0.0982, 0.1089, 0.0943, 0.0912, 0.0996, 0.1020, 0.1021,\n",
      "         0.1056],\n",
      "        [0.0997, 0.1004, 0.0978, 0.1085, 0.0938, 0.0907, 0.0982, 0.1045, 0.1020,\n",
      "         0.1045],\n",
      "        [0.0990, 0.0993, 0.0979, 0.1081, 0.0913, 0.0921, 0.0976, 0.1088, 0.1030,\n",
      "         0.1031],\n",
      "        [0.0955, 0.0996, 0.1010, 0.1081, 0.0931, 0.0913, 0.0988, 0.1044, 0.1031,\n",
      "         0.1051],\n",
      "        [0.0995, 0.0988, 0.0985, 0.1091, 0.0919, 0.0918, 0.0980, 0.1059, 0.1017,\n",
      "         0.1049],\n",
      "        [0.0977, 0.0994, 0.0996, 0.1073, 0.0924, 0.0922, 0.1018, 0.1019, 0.1040,\n",
      "         0.1038],\n",
      "        [0.0972, 0.1011, 0.1010, 0.1062, 0.0931, 0.0908, 0.1006, 0.1048, 0.1037,\n",
      "         0.1016],\n",
      "        [0.0996, 0.0984, 0.0987, 0.1068, 0.0904, 0.0900, 0.0992, 0.1076, 0.1020,\n",
      "         0.1075],\n",
      "        [0.0972, 0.0998, 0.1012, 0.1084, 0.0925, 0.0905, 0.1022, 0.1014, 0.1023,\n",
      "         0.1045],\n",
      "        [0.0979, 0.0998, 0.0985, 0.1080, 0.0959, 0.0910, 0.0979, 0.1049, 0.1027,\n",
      "         0.1033]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "model = MyModel()\n",
    "pred = model(smallx)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred` 的每一行加总为1，其中每一个元素代表对应类别的预测概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以直接打印模型对象，观察隐藏层的结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 损失函数。对于分类问题，损失函数通常选取为负对数似然函数。在 PyTorch 中，可以使用 `torch.nn.NLLLoss` 来完成计算。其用法是先定义一个损失函数对象，然后在预测值和真实标签上调用该函数对象。注意：损失函数对象的第一个参数是预测概率的**对数值**，第二个参数是真实的标签。[文档说明](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3231, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn = torch.nn.NLLLoss()\n",
    "lossfn(torch.log(pred), smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 利用课上介绍的循环模板和代码示例，对模型进行迭代训练。对于本数据，选取 mini-batch 大小为200，共遍历数据10遍，优化器选为 Adam，学习率为0.001。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 |train loss:2.3035948276519775\n",
      "Epoch: 0 |train loss:0.2413109391927719\n",
      "Epoch: 0 |train loss:0.15266233682632446\n",
      "Epoch: 0 |train loss:0.1261148899793625\n",
      "Epoch: 0 |train loss:0.0986529141664505\n",
      "Epoch: 0 |train loss:0.08441997319459915\n",
      "Epoch: 1 |train loss:0.09275760501623154\n",
      "Epoch: 1 |train loss:0.11651147902011871\n",
      "Epoch: 1 |train loss:0.07878801226615906\n",
      "Epoch: 1 |train loss:0.044405557215213776\n",
      "Epoch: 1 |train loss:0.03689614310860634\n",
      "Epoch: 1 |train loss:0.1033848226070404\n",
      "Epoch: 2 |train loss:0.045710016041994095\n",
      "Epoch: 2 |train loss:0.057826463133096695\n",
      "Epoch: 2 |train loss:0.04169587045907974\n",
      "Epoch: 2 |train loss:0.08028850704431534\n",
      "Epoch: 2 |train loss:0.030156051740050316\n",
      "Epoch: 2 |train loss:0.0606003999710083\n",
      "Epoch: 3 |train loss:0.020447397604584694\n",
      "Epoch: 3 |train loss:0.039062898606061935\n",
      "Epoch: 3 |train loss:0.07325118780136108\n",
      "Epoch: 3 |train loss:0.08342142403125763\n",
      "Epoch: 3 |train loss:0.12424413859844208\n",
      "Epoch: 3 |train loss:0.0678277313709259\n",
      "Epoch: 4 |train loss:0.04962542653083801\n",
      "Epoch: 4 |train loss:0.019145430997014046\n",
      "Epoch: 4 |train loss:0.0310664065182209\n",
      "Epoch: 4 |train loss:0.020724304020404816\n",
      "Epoch: 4 |train loss:0.06735200434923172\n",
      "Epoch: 4 |train loss:0.09568293392658234\n",
      "Epoch: 5 |train loss:0.033978886902332306\n",
      "Epoch: 5 |train loss:0.058583613485097885\n",
      "Epoch: 5 |train loss:0.02643280290067196\n",
      "Epoch: 5 |train loss:0.026525506749749184\n",
      "Epoch: 5 |train loss:0.04431980475783348\n",
      "Epoch: 5 |train loss:0.017510458827018738\n",
      "Epoch: 6 |train loss:0.0152200972661376\n",
      "Epoch: 6 |train loss:0.034673403948545456\n",
      "Epoch: 6 |train loss:0.07156427204608917\n",
      "Epoch: 6 |train loss:0.03685842826962471\n",
      "Epoch: 6 |train loss:0.046467043459415436\n",
      "Epoch: 6 |train loss:0.04035089537501335\n",
      "Epoch: 7 |train loss:0.015051960945129395\n",
      "Epoch: 7 |train loss:0.01766836829483509\n",
      "Epoch: 7 |train loss:0.09542884677648544\n",
      "Epoch: 7 |train loss:0.09644313901662827\n",
      "Epoch: 7 |train loss:0.039835769683122635\n",
      "Epoch: 7 |train loss:0.029632756486535072\n",
      "Epoch: 8 |train loss:0.01248643733561039\n",
      "Epoch: 8 |train loss:0.026747548952698708\n",
      "Epoch: 8 |train loss:0.01985296420753002\n",
      "Epoch: 8 |train loss:0.056805774569511414\n",
      "Epoch: 8 |train loss:0.01504429616034031\n",
      "Epoch: 8 |train loss:0.015470333397388458\n",
      "Epoch: 9 |train loss:0.028995657339692116\n",
      "Epoch: 9 |train loss:0.027235718443989754\n",
      "Epoch: 9 |train loss:0.008945029228925705\n",
      "Epoch: 9 |train loss:0.03880223631858826\n",
      "Epoch: 9 |train loss:0.03336377441883087\n",
      "Epoch: 9 |train loss:0.008065959438681602\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "lr = 0.001\n",
    "\n",
    "loader = DataLoader(mnist, batch_size=batch_size, shuffle=True)\n",
    "opimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step,(x,y) in enumerate(loader):\n",
    "        model.train()\n",
    "        b_x = Variable(x)\n",
    "        b_y = Variable(y)\n",
    "        \n",
    "        out = model(b_x)\n",
    "        loss = lossfn(torch.log(out), b_y.long())\n",
    "        opimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        opimizer.step()\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            print('Epoch:',epoch,'|train loss:'+str(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了验证模型的效果，我们对前10个观测（即之前生成的 `smallx` 和 `smally`）进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.999 0.    0.    0.    0.    0.001]\n",
      " [0.    0.    0.    0.    0.    1.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.064 0.    0.936 0.    0.    0.    0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.001 0.001 0.    0.    0.    0.998 0.    0.   ]\n",
      " [0.    0.993 0.    0.    0.001 0.    0.    0.006 0.    0.   ]\n",
      " [1.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    1.    0.    0.   ]\n",
      " [0.    0.    0.    0.    1.    0.    0.    0.    0.    0.   ]]\n",
      "tensor([7, 4, 5, 4, 0, 7, 1, 0, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "ypred = model(smallx)\n",
    "print(np.round(ypred.detach().cpu().numpy(), 3))\n",
    "print(smally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果模型搭建和训练都正常，那么每一行中概率最大的取值所在的位置应该正好对应真实的标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们用模型对一些真实的手写数字图片进行预测。请你利用绘图软件（如 Windows 自带的绘图，或 Photoshop 等）准备10张正方形黑色底色的图片，每张用鼠标绘制一个数字（请使用较粗的笔划），从0到9，然后以0.png，1.png等文件名存储下来，放到当前目录一个名为 digits 的文件夹中。以下是几个例子：\n",
    "![](digits/sample0.png) ![](digits/sample5.png) ![](digits/sample8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来利用 Pillow 软件包读取图片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "name=f\"C:/Users/chenxuanzhu/Downloads/digits/sample0.png\"\n",
    "im = Image.open(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时如果直接将其转为 Numpy 数组会得到三个通道："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 261, 3)\n"
     ]
    }
   ],
   "source": [
    "im_arr = np.array(im)\n",
    "print(im_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们先强制转换为灰度图片（单通道），再缩放至模型的图片大小 28 x 28："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABQElEQVR4nGNgoAlgROaIiop/5vny8cO7vxiSksEuwiyf+SQZ7+yd+4KBgYGBgQUmJRcQp3D/z8ebWmLvNIzlK94h6WQKyNf7+HbT9s+fBf4w2iRKxx5Csi3s9rvjkfJQDs+6l1FIxspksM9acAOmkomP7Q+EwcDAwMDgafxmDlyOge33z1cISSY1hhV3EXboqL5+jSQpwPjtP0JSUODeY4QkI9N/ToQckx7Lva9IdnJ8eIiQ1Av6dP4vQvL3nf/JEjA5sSyFuweQfMlgdO2NL5QpNOfVFXsGFND5Zo0SAwMDAwN74YuH/qhyDDpnPnQwMTAwMCTcu5fPjCbJGHn3ZhQDA3P4rQ/d3AzogL317a32oP4nbxeLYsgxMMivfvL9+ZdX85SQzYOzRAJtxa6d3vgNi0YGBgYGRjYmHDKDBQAAHZJkUSWMm9UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x2DA9BF850A0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = im.convert(\"L\")\n",
    "im.thumbnail((28, 28))\n",
    "im_arr = np.array(im)\n",
    "print(im_arr.shape)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了传递给模型对象，还需要先将数值归一化到 [0,1] 区间，转换为 PyTorch 的 Tensor 类型，并增加一个批次和一个通道的维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "test0 = torch.tensor(im_arr / 255.0, dtype=torch.float32).view(1, 1, 28, 28)\n",
    "print(test0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后对图片标签进行预测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.436 0.    0.395 0.009 0.001 0.    0.006 0.    0.013 0.14 ]]\n"
     ]
    }
   ],
   "source": [
    "pred0 = model(test0)\n",
    "print(np.round(pred0.detach().cpu().numpy(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测结果是否符合真实情形？请对你自己绘制出的10张图片进行类似的预测操作，并评价其效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.122 0.01  0.144 0.111 0.013 0.031 0.009 0.04  0.046 0.474]]\n",
      "[[0.09  0.2   0.073 0.022 0.098 0.199 0.122 0.094 0.072 0.031]]\n",
      "[[0.014 0.04  0.478 0.173 0.034 0.041 0.037 0.078 0.077 0.026]]\n",
      "[[0.009 0.134 0.3   0.181 0.079 0.097 0.003 0.171 0.008 0.017]]\n",
      "[[0.009 0.056 0.015 0.062 0.165 0.291 0.018 0.096 0.086 0.202]]\n",
      "[[0.008 0.006 0.012 0.095 0.004 0.744 0.017 0.019 0.018 0.077]]\n",
      "[[0.559 0.019 0.087 0.018 0.01  0.06  0.168 0.032 0.025 0.022]]\n",
      "[[0.008 0.039 0.061 0.132 0.008 0.018 0.003 0.667 0.03  0.035]]\n",
      "[[0.02  0.009 0.115 0.312 0.004 0.049 0.023 0.019 0.413 0.035]]\n",
      "[[0.015 0.056 0.065 0.104 0.078 0.033 0.003 0.356 0.024 0.267]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "for i in range(10):\n",
    "    name=f\"C:/Users/chenxuanzhu/Downloads/digits/{i}.png\"\n",
    "    im = Image.open(name)\n",
    "    im_arr = np.array(im)\n",
    "    im = im.convert(\"L\")\n",
    "    im.thumbnail((28, 28))\n",
    "    im_arr = np.array(im)\n",
    "    test0 = torch.tensor(im_arr / 255.0, dtype=torch.float32).view(1, 1, 28, 28)\n",
    "    pred0 = model(test0)\n",
    "    print(np.round(pred0.detach().cpu().numpy(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "看第1行0.474可知预测为0错误\n",
    "看第2行0.2可知预测为1正确\n",
    "看第3行0.478可知预测为2正确\n",
    "看第4行0.181可知预测为3错误\n",
    "看第5行0.291可知预测为4错误\n",
    "看第6行0.744可知预测为5正确\n",
    "看第7行0.599可知预测为6错误\n",
    "看第8行0.667可知预测为7正确\n",
    "看第9行0.413可知预测为8正确\n",
    "看第10行0.267可知预测为9正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 目标：通过对英文名数据进行训练，构建一个 RNN 模型，实现英文名的自动生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 主要步骤：获取和整理数据，对字符串进行 one-hot 编码，创建模型结构，定义损失函数，编写训练循环，最后生成人名字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取和整理数据。数据文件已存为 `data/names.txt`，先将其读取为字符串列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668\n",
      "['Abbas', 'Abbey', 'Abbott', 'Abdi', 'Abel']\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "lines = io.open(\"C:/Users/chenxuanzhu/Downloads/data/names.txt\").read().strip().split('\\n')\n",
    "print(len(lines))\n",
    "print(lines[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，共读取了3668个名字。为了简单起见，我们将所有的大写字母转换为小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbas', 'abbey', 'abbott', 'abdi', 'abel']\n"
     ]
    }
   ],
   "source": [
    "names = [s.lower() for s in lines]\n",
    "print(names[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们需要构建一个字符的字典。对于英文名来说很简单，即26个字母。我们可以通过下面的代码直接得到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "dict = string.ascii_lowercase\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 下面准备好 one-hot 编码所需的函数。编写函数 `char2index(char)`，将一个字母转换为其所在字典的位置。例如 `char2index(\"a\")` 要返回0，`char2index(\"z\")` 要返回25，等等。提示：使用字符串的 `.find()` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def char2index(char):\n",
    "    return dict.find(char)\n",
    "\n",
    "print(char2index(\"z\") == 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写 `char2tensor(char)` 函数，将一个字母转换为 one-hot 向量，即该向量中第 i 个元素为1，其余为0，其中 i 表示该字母在字典中的位置。\n",
    "\n",
    "**注意，该向量的长度应为27，因为我们要预留终止符，用 `[0.0, 0.0, ..., 1.0]` 表示**。\n",
    "\n",
    "`char2tensor(\"a\")` 应返回 `torch.tensor([1.0, 0.0, ...])`，`char2tensor(\"z\")` 应返回 `torch.tensor([0.0, ..., 1.0, 0.0])`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def char2tensor(char):\n",
    "    tensor = torch.zeros(1, 27)\n",
    "    tensor[0, char2index(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(char2tensor(\"a\"))\n",
    "print(char2tensor(\"z\"))\n",
    "print(char2tensor(\"z\").shape[0] == 27)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 构建模型。我们使用最简单的 RNN 结构，即隐藏单元是输入和上一期隐藏单元的线性变换加上 Tanh 激活函数，输出单元是隐藏单元的线性变换加上 Softmax 激活函数。输出的结果代表下一个字符的概率分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = ...\n",
    "        self.h2o = ...\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = torch.nn.functional.softmax(self.h2o(hidden), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.h2o = torch.nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        hidden = torch.tanh(self.i2h(combined))\n",
    "        output = torch.nn.functional.softmax(self.h2o(hidden), dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们做一个简单的测试。请在下面的代码中加入适当的语句，使得每次运行的结果不变。根据其输出结果，请问当前模型预测字符a的下一个字符是什么？为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0290, 0.0327, 0.0320, 0.0354, 0.0432, 0.0395, 0.0417, 0.0371, 0.0409,\n",
      "         0.0275, 0.0328, 0.0359, 0.0373, 0.0391, 0.0339, 0.0366, 0.0430, 0.0364,\n",
      "         0.0472, 0.0435, 0.0376, 0.0327, 0.0418, 0.0370, 0.0279, 0.0426, 0.0357]],\n",
      "       grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(123)\n",
    "device = torch.device(\"cpu\")\n",
    "rnn = RNN(input_size=27, hidden_size=10)\n",
    "input = char2tensor(\"a\")\n",
    "hidden = rnn.init_hidden()\n",
    "output, hidden = rnn(input.view(1, 27), hidden)\n",
    "print(output)\n",
    "torch.max(output,1)[1].data.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 接下来我们定义好损失函数。与第1题中类似，预测值是一个概率分布，而真实的标签是0到26中的一个整数，代表真实的下一个字符在字典中的位置。假设当前处理的名字为\"abel\"，那么字符a的输出结果对应的标签是什么？请完成下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4199, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Change \"target\" to a proper value\n",
    "target = char2index(\"b\")\n",
    "\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "loss = lossfn(torch.log(output), torch.tensor([target]))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 明确单个字符的损失函数的计算方法后，请在下面计算出\"abel\"这个观测整体的损失函数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2926, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "chac = list(\"abel\")\n",
    "loss = 0\n",
    "for i in range(len(chac)):\n",
    "    input = char2tensor(chac[i])\n",
    "    hidden = rnn.init_hidden()\n",
    "    output, hidden = rnn(input.view(1, 27), hidden)\n",
    "    target = char2index(chac[i+1]) if i!= len(chac)-1 else  26\n",
    "    loss += lossfn(torch.log(output), torch.tensor([target]))\n",
    "loss = loss/len(chac)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 将上述过程在数据上进行反复迭代，训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, obs 0, loss = 3.2788641452789307\n",
      "epoch 0, obs 1000, loss = 2.793740749359131\n",
      "epoch 0, obs 2000, loss = 2.8677024841308594\n",
      "epoch 0, obs 3000, loss = 2.556673765182495\n",
      "epoch 1, obs 0, loss = 2.695556640625\n",
      "epoch 1, obs 1000, loss = 2.5480618476867676\n",
      "epoch 1, obs 2000, loss = 2.6293277740478516\n",
      "epoch 1, obs 3000, loss = 2.432743549346924\n",
      "epoch 2, obs 0, loss = 2.6687285900115967\n",
      "epoch 2, obs 1000, loss = 2.425450325012207\n",
      "epoch 2, obs 2000, loss = 2.685854911804199\n",
      "epoch 2, obs 3000, loss = 2.7870194911956787\n",
      "epoch 3, obs 0, loss = 2.547203540802002\n",
      "epoch 3, obs 1000, loss = 2.3284201622009277\n",
      "epoch 3, obs 2000, loss = 2.8587934970855713\n",
      "epoch 3, obs 3000, loss = 3.0973093509674072\n",
      "epoch 4, obs 0, loss = 2.2342605590820312\n",
      "epoch 4, obs 1000, loss = 2.7695655822753906\n",
      "epoch 4, obs 2000, loss = 2.6228115558624268\n",
      "epoch 4, obs 3000, loss = 2.2542669773101807\n",
      "epoch 5, obs 0, loss = 2.3726961612701416\n",
      "epoch 5, obs 1000, loss = 2.792773962020874\n",
      "epoch 5, obs 2000, loss = 2.6232218742370605\n",
      "epoch 5, obs 3000, loss = 1.7685332298278809\n",
      "epoch 6, obs 0, loss = 2.113428831100464\n",
      "epoch 6, obs 1000, loss = 2.244549512863159\n",
      "epoch 6, obs 2000, loss = 2.509845733642578\n",
      "epoch 6, obs 3000, loss = 2.647892713546753\n",
      "epoch 7, obs 0, loss = 2.7292425632476807\n",
      "epoch 7, obs 1000, loss = 1.9779046773910522\n",
      "epoch 7, obs 2000, loss = 2.4551761150360107\n",
      "epoch 7, obs 3000, loss = 2.0647366046905518\n",
      "epoch 8, obs 0, loss = 2.235952615737915\n",
      "epoch 8, obs 1000, loss = 2.1242618560791016\n",
      "epoch 8, obs 2000, loss = 2.0623395442962646\n",
      "epoch 8, obs 3000, loss = 2.236877202987671\n",
      "epoch 9, obs 0, loss = 2.5386266708374023\n",
      "epoch 9, obs 1000, loss = 2.335432767868042\n",
      "epoch 9, obs 2000, loss = 2.270106315612793\n",
      "epoch 9, obs 3000, loss = 3.0703208446502686\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "torch.random.manual_seed(123)\n",
    "\n",
    "n = len(names)\n",
    "n_hidden = 16\n",
    "n_input = 27\n",
    "nepoch = 10\n",
    "train_ind = np.arange(n)\n",
    "\n",
    "rnn = RNN(n_input, n_hidden)\n",
    "opt = torch.optim.Adam(rnn.parameters(), lr=0.0001)\n",
    "train_ind = np.arange(n)\n",
    "losses = []\n",
    "\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "\n",
    "# Loop over epochs\n",
    "for k in range(nepoch):\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(train_ind)\n",
    "    # Loop over observations. Each observation is a name\n",
    "    for i in range(n):\n",
    "        name = names[train_ind[i]]\n",
    "        nchar = len(name)\n",
    "        # Loop over the characters in the name\n",
    "        # Each input character has a target, which is the index of the next character in the dictionary\n",
    "        # For the last character in the name, the target is the end-of-sequence symbol, which has index 26\n",
    "        loss = 0.0\n",
    "        hidden = rnn.init_hidden()\n",
    "        for j in range(nchar):\n",
    "            input = char2tensor(name[j])\n",
    "            output, hidden = rnn(input.view(1, n_input), hidden)\n",
    "\n",
    "            if j == nchar - 1:\n",
    "                target = 26\n",
    "            else:\n",
    "                target = char2index(name[j + 1])\n",
    "\n",
    "            loss = loss + lossfn(torch.log(output), torch.tensor([target]))\n",
    "    \n",
    "        loss = loss / nchar\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"epoch {k}, obs {i}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2da93a1e6d0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 编写一个函数 `random_first_letter()`，它随机返回字典中的一个字符，我们将利用它来随机生成第一个字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "k\n",
      "s\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "def random_first_letter():\n",
    "    # Implementation here\n",
    "    return np.random.choice(list(dict), 1)[0]\n",
    "\n",
    "print(random_first_letter())\n",
    "print(random_first_letter())\n",
    "print(random_first_letter())\n",
    "\n",
    "#随机生成第一个字符\n",
    "first_letter = random_first_letter()\n",
    "char_ind = [char2index(first_letter)]\n",
    "print(char_ind)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请简要说明如下的代码的含义（可以在代码中加入注释），然后利用它随机生成10个名字。评价生成的结果，并简要说明可以如何改进模型的效果？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_name(max_len=20): #生成一个最大长度为20名字的函数\n",
    "    rnn.eval() \n",
    "    first_letter = random_first_letter()#随机选取第一个字母\n",
    "    char_ind = [char2index(first_letter)]\n",
    "    input = char2tensor(first_letter)#将第一个字母转化为tensor输入模型\n",
    "    hidden = rnn.init_hidden()\n",
    "    for i in range(max_len - 1):\n",
    "        output, hidden = rnn(input.view(1, n_input), hidden) #输入网络\n",
    "        ind = torch.argmax(output).item()\n",
    "        if ind == 26:  #如果下一个为停止符号，就结束生成\n",
    "            break\n",
    "        char_ind.append(ind)\n",
    "        input.zero_()\n",
    "        input[0,ind] = 1.0\n",
    "    return \"\".join([dict[i] for i in char_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garlen\n",
      "tone\n",
      "warlen\n",
      "erer\n",
      "warlen\n",
      "urer\n",
      "tone\n",
      "qarlen\n",
      "harlen\n",
      "erer\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): \n",
    "    print(random_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##通过大部分名字感觉不太熟用，出现了多次arlen、erlen这种，可能是因为模型是通过一个字母预测下一个字母，可以通过改进模型使得用已有的全部名字去估计下一个字母。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用卷积函数实现任意大整数的乘法。给定两个整数，如 183612 和 23333，用两个列表表达它们的序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = [1, 8, 3, 6, 1, 2]\n",
    "n2 = [2, 3, 3, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请编写一个函数 `integer_mult(n1, n2)`，返回 `n1 * n2` 对应的整数序列。注意不要直接调用乘法表达式（设想有两个非常大的整数，直接相乘可能会导致数值溢出）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 19 33 48 56 58 36 27  9  6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def integer_mult(n1, n2):\n",
    "    a = np.array(n1)\n",
    "    b = np.array(n2)\n",
    "    return(np.convolve(a,b))\n",
    "    print(poly_mult(p, q))\n",
    "print(integer_mult(n1, n2))\n",
    "\n",
    "#res = integer_mult(n1, n2)\n",
    "#print(res == [4, 2, 8, 4, 2, 1, 8, 7, 9, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 先实现多项式的乘法。例如，给定 $p(x)=1+2x+x^4$ 和 $q(x)=x+3x^2+5x^3$，计算 $r(x)=p(x)q(x)$。我们将 $p(x)$ 编码为 `p = [1, 2, 0, 0, 1]`，$q(x)$ 编码为 `q = [0, 1, 3, 5]`，请编写函数 `poly_mult(p, q)`，使得 `poly_mult(p, q) == [0, 1, 5, 11, 10, 1, 3, 5]`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 对于任意的一个整数，将其看成是某个多项式在 $x=10$ 处的取值，如 $123 = p_1(10)$，$p_1(x)=3+2x+x^2$，$5310 = p_2(10)$，$p_2(x)=x+3x^2+5x^3$，注意需要适当将序列反序。因此，要计算 $123\\times 5310$，相当于计算 $r(10)$ 的值，但为了避免直接进行乘法运算（防止溢出），可以先计算 $r(x)$ 的表达式（等价于其系数向量），然后建立起 $r(x)$ 的系数与 $r(10)$ 之间的联系（见如下第3点）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如果一个多项式 $r(x)$ 所有的系数都是0到9之间的整数，那么 $r(x)$ 和 $r(10)$ 的关系非常直接，比如若 $r(x)=1+2x+5x^2+3x^3$，则 $r(10)=3521$。但如果有系数超过10，就需要考虑进位的影响，比如 $r(x)=1+11x+2x^2$，$r(10)=311$。此时可以从 $r(x)$ 的第一项开始逐项进位，构造一个新的多项式 $r'(x)=1+x+3x^2$，满足 $r'(10)=r(10)$，且 $r'(x)$ 所有的系数都不超过10。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_poly(ans):\n",
    "    for i in range(len(ans)-1):\n",
    "        out = ans[i]//10\n",
    "        stay = ans[i]%10\n",
    "        ans[i+1] += out\n",
    "        ans[i] = stay\n",
    "    b = ans[-1]\n",
    "    ans.pop(-1)\n",
    "    while(b//10 >0):\n",
    "        ans.append(b%10)\n",
    "        b //= 10\n",
    "    ans.append(b)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 综合利用以上信息，完成本题的算法编写。并测试 23742389754298365 * 809723950 的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 9.0, 2.0, 2.0, 4.0, 7.0, 8.0, 1.0, 6.0, 1.0, 4.0, 2.0, 9.0, 0.0, 0.0, 0.0, 1.0, 5.0, 8.0, 6.0, 3.0, 4.0, 1.0, 7.0, 5.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "l1 = [int(x) for x in '23742389754298365']\n",
    "l2 = [int(x) for x in '809723950']\n",
    "def integer_mult(n1, n2):\n",
    "    ans = poly_mult(n1, n2)\n",
    "    return new_poly(ans)\n",
    "\n",
    "res = list(reversed(integer_mult(list(reversed(l1)), list(reversed(l2)))))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
